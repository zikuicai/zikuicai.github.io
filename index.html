<!DOCTYPE HTML>
<html lang="en">
  
  <head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Zikui Cai</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <script src="script.js" defer></script>
  </head>


  <body>
    <table width="900" border="0" align="center" cellspacing="0" cellpadding="0">
        <tbody>
            <tr>
                <td>
                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody><tr>
                            <td width="60%" valign="middle">
                                <p align="center">
                                    <name>Zikui Cai</name>
                                </p>

                                <!-- <p align="center"> -->
                                <p>
                                  I am currently a Ph.D. student in <a href="https://www.ece.ucr.edu/">Electrical and Computer Engineering</a>  at UC Riverside, 
                                  advised by Professor <a href="https://intra.ece.ucr.edu/~sasif/">M. Salman Asif</a>. <br>
                                  My research interests broadly span the area of computer vision, computational imaging, and (adversarial) machine learning.
                                </p>
                                <p align="center">
                                  <a href="mailto:zcai032@ucr.edu"> <img src="figs/icon-email.svg", alt="Email", width="24"> </a>
                                  &emsp; ᕕ( ᐛ )ᕗ &emsp;
                                  <a href="https://github.com/zikuicai"> <img src="figs/icon-github.svg", alt="GitHub"></a>
                                  &emsp; (ง •_•)ง &emsp;
                                  <a href="https://scholar.google.com/citations?user=-SrU69AAAAAJ&hl=en&oi=ao"> <img src="figs/icon-google-scholar.svg", alt="Google Scholar"></a>
                                  &emsp;&emsp;
                                </p>
                            </td>
                            <td width="30%">
                              <img src="figs/selfie_circle.png">
                            </td>
                        </tr>
                        </tbody>
                    </table>


          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td>
                <heading>Publications</heading>
              </td> 
            </tr>
          </table>

          <!-- paper 2022 AAAI-->
          <table width="100%" align="center" border="0" cellpadding="20">
            <tr>
              <td width="25%" valign="center">
                <img src="figs/cai2022context.png" alt="fig" width="250">
              </td>

              <td width="75%" valign="top">
                <p>
                  <papertitle>Context-Aware Transfer Attacks for Object Detection</papertitle><br>
                  <author>Zikui Cai, Xinxin Xie, Shasha Li, Mingjun Yin, Chengyu Song, Srikanth V. Krishnamurthy, Amit K. Roy-Chowdhury, M. Salman Asif</author><br>
                  <venue>AAAI Conference on Artificial Intelligence (AAAI), 2022</venue><br>
                    <a href="https://arxiv.org/abs/2112.03223"> <resource>paper</resource> </a> / 
                    <a href="https://github.com/CSIPlab/context-aware-attacks"> <resource>code</resource> </a> / 
                    <a href="https://github.com/CSIPlab/context-aware-attacks/blob/main/doc/slides.pdf"> <resource>slides</resource> </a> / 
                    <a href="https://github.com/CSIPlab/context-aware-attacks/blob/main/doc/poster.pdf"> <resource>poster</resource> </a> / 
                    <a href="https://aaai-2022.virtualchair.net/poster_aaai6996"> <resource>video</resource> </a>
                  <p> TL;DR. We transfer attack black-box object detectors by leveraging context information, including co-occurrence of objects and their relative locations and sizes. </p>
                </p>
              </td>
              
            </tr>
          </table>

          <!-- paper 2021 ICCV-->
          <table width="100%" align="center" border="0" cellpadding="20">
            <tr>
              <td width="25%" valign="center">
                <img src="figs/yin2021exploiting.png" alt="fig" width="250">
              </td>

              <td width="75%" valign="top">
                <p>
                  <papertitle>Exploiting Multi-Object Relationships for Detecting Adversarial Attacks in Complex Scenes</papertitle><br>
                  <author>Mingjun Yin, Shasha Li, Zikui Cai, Chengyu Song, M. Salman Asif, Amit K. Roy-Chowdhury, Srikanth V. Krishnamurthy</author><br>
                  <venue>International Conference on Computer Vision (ICCV), 2021</venue><br>
                    <a href="https://arxiv.org/abs/2108.08421"> <resource>paper</resource> </a>
                  <p> TD;LR. We developed a novel approach (SCENE-Lang) to perform model-agnostic context consistency checks using language models. </p>
                </p>
              </td>
              
            </tr>
          </table>

          <!-- paper 2021 ICIP-->
          <table width="100%" align="center" border="0" cellpadding="20">
            <tr>
              <td width="25%" valign="center">
                <img src="figs/cai2021data.png" alt="fig" width="250">
              </td>

              <td width="75%" valign="top">
                <p>
                  <papertitle>Data-Driven Illumination Patterns For Coded Diffraction Imaging</papertitle><br>
                  <author>Zikui Cai, Rakib Hyder, M. Salman Asif</author><br>
                  <venue>IEEE International Conference on Image Processing (ICIP), 2021</venue><br>
                    <a href="https://ieeexplore.ieee.org/abstract/document/9506350"> <resource>paper</resource> </a>
                  <p> TD;LR. We present a framework to optimize the sensing parameters to recover signals from coded diffraction patterns using alternating minimization-based phase retrieval method. </p>
                </p>
              </td>
              
            </tr>
          </table>

          <!-- paper 2020 ECCV -->
          <table width="100%" align="center" border="0" cellpadding="20">
            <tr>
              <td width="25%" valign="center">
                <img src="figs/hyder2020solving.png" alt="fig" width="250">
              </td>

              <td width="75%" valign="top">
                <p>
                  <papertitle>Solving Phase Retrieval with a Learned Reference</papertitle>
                  <br> 
                  <author>Rakib Hyder*, Zikui Cai*, M. Salman Asif</author><br>
                  (* co-first authorship))<br>
                  <venue>European Conference on Computer Vision (ECCV), 2020</venue><br>
                    <a href="https://arxiv.org/abs/2007.14621"> <resource>paper</resource> </a> /
                    <a href="https://github.com/CSIPlab/learn-reference-pr"> <resource>code</resource> </a>
                  <p> TD;LR. Our proposed method for phase retrieval via unrolled network and learned reference provides near-perfect recovery at fixed (small) computational cost. </p>
                </p>
              </td>

            </tr>
          </table>

          <!--Project Starts-->
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td>
                <heading>Projects</heading>
              </td>
            </tr>
          </table>

          <!-- 5 -->
          <table width="100%" align="center" border="0" cellpadding="20">
            <tr>
              <td width="25%" valign="center">
                <img src="figs/deepfakes.png" alt="deepfakes" width="250">
              </td>

              <td width="75%" valign="top">
                <p>
                  <papertitle>Deepfakes Image Generation</papertitle>
                  <br> with Jiatai Chen and Taanya Gupta
                  <!-- <br> <a href="https://github.com/zikuicai/deepfakes"> code </a> -->
                  <p>We performed deepfakes face image translation using a encoder-decoder structure. Trained a CycleGAN with face segmentation maps as guides.</p>
                </p>
              </td>
            </tr>
          </table>

          <!-- 4 -->
          <table width="100%" align="center" border="0" cellpadding="20">
            <tr>
              <td width="25%" valign="center">
                <img src="figs/handpose.jpg" alt="handpose" width="250">
              </td>

              <td width="75%" valign="top">
                <p>
                  <papertitle>Hand Pose Estimation</papertitle></a>
                  <!-- <br><a href="https://github.com/zikuicai/HandPoseEstimation"> code </a> -->

                  <p>
                    Developed an algorithm for real-time human 2D hand pose estimation.
                    Accelerated the postprocess by executing gaussian filtering operation in Keras model.

                  </p>
                </p>
              </td>
            </tr>
          </table>
          
          <!-- 3 -->
          <table width="100%" align="center" border="0" cellpadding="20">
            <tr>
              <td width="25%" valign="center">
                <img src="figs/dance.png" width="250">
              </td>

              <td width="75%" valign="top">
                <p>
                  <!-- <a href="https://github.com/Rivenlalala/StreetDanceRecognition"></a> -->
                  <papertitle>2D Skeleton-Based Dance Genre Recognition</papertitle>
                  
                  <br>with Ruiwen Zhao and Xiaodi Fan
                  <p>
                    Made a dataset by mining massive street dance videos from Youtube. 
                    Classified videos using CNN & RNN based on extracted human body skeleton. 
                    Generalized the prior arts to solve real life problems in 2D space.
                  </p>
                </p>
              </td>
            </tr>
          </table>

          <!-- 2 -->
          <table width="100%" align="center" border="0" cellpadding="20">
            <tr>
              <td width="25%" valign="center">
                <img src="figs/pose.png" width="250">
              </td>

              <td width="75%" valign="top">
                <p>
                  <!-- <a href="https://drive.google.com/open?id=1zV8Wbvy1sEgVlpxSkA9L55obyp2X7ZRxgN8-m0JF4K8"> --></a>
                  <papertitle>Human Pose Estimation</papertitle>
                  
                  <br> with Ruiwen Zhao</a>
                  <p>
                    Built an VGGNet based 2-branch multistage CNN model using Keras. Trained and validated the model on COCO dataset and got a good result.
                  </p>
                </p>
              </td>
            </tr>
          </table>

          <!-- 1 -->
          <table width="100%" align="center" border="0" cellpadding="20">
            <tr>
              <td width="25%" valign="center">
                <img src="figs/car.png" width="250">
              </td>

              <td width="75%" valign="top">
                <p>
                  <!-- <a href="https://drive.google.com/open?id=1xesAvtPhNJ-XcRzJO5gUmk0bQaqMlOwI96AxWlh6y2I"></a> -->
                  <papertitle>Autonomous Vehicles</papertitle>
                  <br> with Vu Tran
                  <p>
                    We designed an autonomuous system where the vehicle can navigate itself under controlled settings. The vehicle is controlled using Raspberry Pi 3B and Arduino.
                    And the images are processed real-time using CNN models in OpenCV.
                  </p>
                </p>
              </td>
            </tr>
          </table>


        <p align="right">
          <!-- <font size="-1"> -->
          <a href="https://github.com/jonbarron/website" target="_blank" style="font-size:14px;">Thanks Jon Barron for the Template</a>
          </font>
        </p>
                </td>
            </tr>
        </tbody>
    </table>


  </body>
</html>